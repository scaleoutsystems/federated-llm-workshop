{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from trl import SFTTrainer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the *CARDBiomedBench* dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more information about the dataset at **[Hugging Face](https://huggingface.co/datasets/NIH-CARD/CARDBiomedBench)**, or by reading the [paper](https://www.biorxiv.org/content/10.1101/2025.01.15.633272v2.full.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"NIH-CARD/CARDBiomedBench\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains question-answer pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = dataset[\"train\"][0][\"question\"]\n",
    "answer = dataset[\"train\"][0][\"answer\"]\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer  : \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's only use data samples related to category \"Pharmacology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"bio_category\"] == \"Pharmacology\")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# for computational resons, select a smaller subset\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(1000))\n",
    "test_dataset = test_dataset.shuffle(seed=42).select(range(200))\n",
    "\n",
    "print(\"Num samples in train set: \", len(train_dataset))\n",
    "print(\"Num samples in test set : \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = train_dataset[0][\"question\"]\n",
    "answer = train_dataset[0][\"answer\"]\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer  : \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the [SmolLM2](https://huggingface.co./HuggingFaceTB/SmolLM2-135M) decoder model, developed by Hugging Face. The SmolLM2 models come in three sizes (135M, 360M, and 1.7B parameters) and are developed to solve a wide range of tasks while being lightweight enough to run on-device.\n",
    "Here, we choose the 135M parameter model for computational reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the model and tokenizer through Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(model).to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # set end-of-sequence token as padding token\n",
    "model.config.pad_token_id = model.config.eos_token_id  # tell model which token to use for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.num_parameters() # 134,515,008 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate an example output of our base LLM.\n",
    "\n",
    "Note that the model is not instruction-tuned (unlike ChatGPT). It is only trained to predict the next token in a sequence and is less useful for interactive tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"The capital of Sweden is \", return_tensors=\"pt\", padding=True).to(device)\n",
    "outputs = model.generate(inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though 135M parameters is relatively \"small\" for an LLM, we want to further reduce the number of trainable parameters through LoRA. \n",
    "\n",
    "This becomes even more necessary when we choose to fine-tune larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By training only 0.34â€¯% of the total parameters, we update only a small fraction of the total parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the SmolLM2 base model on some questions from our dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(index, dataset, model, tokenizer):\n",
    "    data = dataset[index]\n",
    "    question = data[\"question\"]\n",
    "\n",
    "    instruction = \"You are a knowledgeable assistant. Answer this question truthfully!\"\n",
    "\n",
    "    # format the input into instruction format\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        f\"{instruction}\\n\\n\"\n",
    "        \"### Input:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    # tokenize prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # generate response\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=100,    \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "\n",
    "    # decode the response & remove special tokens\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # remove prompt from response\n",
    "    response = response[len(prompt):]\n",
    "\n",
    "    expected_response = data[\"answer\"]\n",
    "\n",
    "    return question, response, expected_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "\n",
    "question, response, expected_response = test_model(index, train_dataset, model, tokenizer)\n",
    "\n",
    "print(\"question: \\n\", question, \"\\n\")\n",
    "print(\"model output: \\n\", response)\n",
    "print(\"expected output: \\n\", expected_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralized Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to fine-tune the model on the train dataset. For this, we convert the training data to instruction format. This is the correct format for generative question-answering tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instruction_format(example):\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "\n",
    "    instruction = \"You are a knowledgeable assistant. Answer this question truthfully!\"\n",
    "\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        f\"{instruction.strip()}\\n\\n\"\n",
    "        \"### Input:\\n\"\n",
    "        f\"{question.strip()}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "        f\"{answer.strip()}\" + tokenizer.eos_token\n",
    "    )\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_train_dataset = train_dataset.map(\n",
    "    generate_instruction_format,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    batched=False,\n",
    ")\n",
    "mapped_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"cuda \", use_cuda)\n",
    "\n",
    "model.train()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,\n",
    "    logging_steps=20,\n",
    "    save_total_limit=2,\n",
    "    use_cpu=not(use_cuda)\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=mapped_train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check some outputs of our fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1\n",
    "\n",
    "question, response, expected_response = test_model(index, train_dataset, model, tokenizer)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"question: \", question, \"\\n\")\n",
    "print(\"model output: \", response, \"\\n\")\n",
    "print(\"expected output: \", expected_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "\n",
    "question, response, expected_response = test_model(index, test_dataset, model, tokenizer)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"question: \", question, \"\\n\")\n",
    "print(\"model output: \", response, \"\\n\")\n",
    "print(\"expected output: \", expected_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we cannot check every output individually. \n",
    "\n",
    "\n",
    "Instead, we use **ROUGE-L** as a metric to evaluate the fine-tuned model on the test dataset.\n",
    "\n",
    "The **ROUGE-L** score is based on the longest common subsequence (LCS) between the generated and the reference text. \n",
    "The LCS is the longest sequence of words that appear in order in both generated and reference text. \n",
    "The words do **NOT** need to be contiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "- *Reference text:* \"The **kid** is **playing** with **the cat**\"\n",
    "\n",
    "- *Generated text:* \"kid playing the cat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that case, the *LCS* is \"kid playing the cat\". \n",
    "\n",
    "To calculate the ROUGE-L score, we need the following information:\n",
    "\n",
    "- length(LCS) = 4\n",
    "- length(reference text) = 7\n",
    "- length(generated text) = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one can calculate Recall, Precision, and F1 Score\n",
    "\n",
    "- *ROUGE-L Recall* = $\\frac{\\text{length(LCS)}}{\\text{length(reference text)}} = \\frac{4}{7} \\approx 0.57$\n",
    "\n",
    "- *ROUGE-L Precision* = $\\frac{\\text{length(LCS)}}{\\text{length(generated text)}} = \\frac{4}{4} = 1.0 $\n",
    "\n",
    "- *ROUGE-L F1 Score* = $\\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\approx 0.73 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "reference_text = [\"The kid is playing with the cat\"]\n",
    "generated_text = [\"kid playing the cat\"]\n",
    "\n",
    "results = rouge.compute(predictions=generated_text, references=reference_text)\n",
    "\n",
    "rouge_l = float(round(results[\"rougeL\"], 2))\n",
    "print(\"rouge_l score: \", rouge_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that generates model predictions on the test dataset and returns them together with the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(example):\n",
    "    instruction = \"You are a knowledgeable assistant. Answer this question truthfully!\"\n",
    "\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    # format the input the same way as during training\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        f\"{instruction}\\n\\n\"\n",
    "        \"### Input:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    # tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "        )\n",
    "\n",
    "    # decode and clean up the response\n",
    "    predicted_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # remove the prompt from the response\n",
    "    predicted_output = predicted_output[len(prompt):]\n",
    "\n",
    "    return {\n",
    "        \"question\": example[\"question\"],\n",
    "        \"predicted_output\": predicted_output,\n",
    "        \"correct_output\": example[\"answer\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computational reasons, we only evaluate the model on a small subset of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_dataset.select(range(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dataset = test_ds.map(get_predictions, batched=False, remove_columns=test_ds.column_names)\n",
    "predictions_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions_dataset[\"predicted_output\"]\n",
    "references = predictions_dataset[\"correct_output\"]\n",
    "\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(f\"ROUGE-L (F1): {results['rougeL']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to SmolLM2 base model (not fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload SmolLM2 base model\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "untrained_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_untrained_predictions(example):\n",
    "    instruction = \"You are a knowledgeable assistant. Answer this question truthfully!\"\n",
    "\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    # Format the input the same way as during training\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        f\"{instruction}\\n\\n\"\n",
    "        \"### Input:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = untrained_model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.2) \n",
    "    \n",
    "    predicted_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predicted_output = predicted_output[len(prompt):]\n",
    "\n",
    "    return {\n",
    "        \"question\": example[\"question\"],\n",
    "        \"predicted_output\": predicted_output,\n",
    "        \"correct_output\": example[\"answer\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_predictions_dataset = test_ds.map(get_untrained_predictions, batched=False, remove_columns=test_ds.column_names)\n",
    "untrained_predictions_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = untrained_predictions_dataset[\"predicted_output\"]\n",
    "references = untrained_predictions_dataset[\"correct_output\"]\n",
    "\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(f\"ROUGE-L (F1): {results['rougeL']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE-L (F1) of fine-tuned model: ~ 70% :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
