{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edgelab/llm-workshop/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from trl import SFTTrainer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the *CARDBiomedBench* dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more information about the dataset at **[Hugging Face](https://huggingface.co/datasets/NIH-CARD/CARDBiomedBench)**, or by reading the [paper](https://www.biorxiv.org/content/10.1101/2025.01.15.633272v2.full.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['uuid', 'template_uuid', 'question', 'answer', 'bio_category', 'reasoning_category'],\n",
       "        num_rows: 58079\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['uuid', 'template_uuid', 'question', 'answer', 'bio_category', 'reasoning_category'],\n",
       "        num_rows: 10148\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"NIH-CARD/CARDBiomedBench\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains question-answer pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What genes does Olaparib target when it's used to treat cancer?\n",
      "Answer  :  When used to treat cancer, the drug Olaparib targets the genes PARP2, PARP1, and PARP3.\n"
     ]
    }
   ],
   "source": [
    "question = dataset[\"train\"][0][\"question\"]\n",
    "answer = dataset[\"train\"][0][\"answer\"]\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer  : \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's only use data samples related to category \"Pharmacology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num samples in train set:  1000\n",
      "Num samples in test set :  200\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"bio_category\"] == \"Pharmacology\")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# for computational resons, select a smaller subset\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(1000))\n",
    "test_dataset = test_dataset.shuffle(seed=42).select(range(200))\n",
    "\n",
    "print(\"Num samples in train set: \", len(train_dataset))\n",
    "print(\"Num samples in test set : \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the [SmolLM2](https://huggingface.co./HuggingFaceTB/SmolLM2-135M) decoder model, developed by Hugging Face. The SmolLM2 models come in three sizes (135M, 360M, and 1.7B parameters) and are developed to solve a wide range of tasks while being lightweight enough to run on-device.\n",
    "Here, we choose the 135M parameter model for computational reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the model and tokenizer through Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(model).to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # set end-of-sequence token as padding token\n",
    "model.config.pad_token_id = model.config.eos_token_id  # tell model which token to use for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134515008"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters() # 134,515,008 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate an example output of our base LLM.\n",
    "\n",
    "Note that the model is not instruction-tuned (unlike ChatGPT). It is only trained to predict the next token in a sequence and is less useful for interactive tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Sweden is  the city of Stockholm. The city is located in the middle of the country, and is the\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"The capital of Sweden is \", return_tensors=\"pt\", padding=True).to(device)\n",
    "outputs = model.generate(inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though 135M parameters is relatively \"small\" for an LLM, we want to further reduce the number of trainable parameters through LoRA. \n",
    "\n",
    "This becomes even more necessary when we choose to fine-tune larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 460,800 || all params: 134,975,808 || trainable%: 0.3414\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,                                    # rank of low-rank decomposition matrix\n",
    "    lora_alpha=16,                          # the higher, the more aggressive fine-tuning will be\n",
    "                                            # effective learning rate: (lora_alpha/r) * learning_rate\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],    # layers in transformer to apply LoRA to\n",
    "    lora_dropout=0.1,                       # prevents overfitting\n",
    "    task_type=TaskType.CAUSAL_LM,           # for autoregressive models\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By training only 0.34 % of the total parameters, we update only a small fraction of the total parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the SmolLM2 base model on some questions from our dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(index, dataset, model, tokenizer):\n",
    "    data = dataset[index]\n",
    "    question = data[\"question\"]\n",
    "\n",
    "    instruction = \"You are a knowledgeable assistant. Answer this question truthfully!\"\n",
    "\n",
    "    # format the input into instruction format\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        f\"{instruction}\\n\\n\"\n",
    "        \"### Input:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    # tokenize prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # generate response\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=100,    \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "\n",
    "    # decode the response & remove special tokens\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # remove prompt from response\n",
    "    response = response[len(prompt):]\n",
    "\n",
    "    expected_response = data[\"answer\"]\n",
    "\n",
    "    return question, response, expected_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: \n",
      " What type of molecule is Leuprolide Mesylate, and what is its action type? \n",
      "\n",
      "model output: \n",
      " Leptolipin (LPL) has an active site that binds to the hydrophobic side chains in phospholipids such as cholesterol or triglycerides. This binding allows for lipophilic molecules like leprous bacteria to be transported into cells where they can multiply rapidly without being destroyed by phagocytes. The LPO-binding sites on these bacterial membranes also allow them to bind with other proteins which then act upon their own membrane lipids causing cell death when released from host tissues through endocytosis processes.\n",
      "\n",
      "expected output: \n",
      " Leuprolide Mesylate is a protein drug that acts as an agonist.\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "question, response, expected_response = test_model(index, train_dataset, model, tokenizer)\n",
    "\n",
    "print(\"question: \\n\", question, \"\\n\")\n",
    "print(\"model output: \\n\", response)\n",
    "print(\"expected output: \\n\", expected_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralized Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to fine-tune the model on the train dataset. For this, we convert the training data to instruction format. This is the correct format for generative question-answering tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instruction_format(example):\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "\n",
    "    instruction = \"You are a knowledgeable assistant. Answer this question truthfully!\"\n",
    "\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        f\"{instruction.strip()}\\n\\n\"\n",
    "        \"### Input:\\n\"\n",
    "        f\"{question.strip()}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "        f\"{answer.strip()}\" + tokenizer.eos_token\n",
    "    )\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '### Instruction:\\nYou are a knowledgeable assistant. Answer this question truthfully!\\n\\n### Input:\\nWhat type of molecule is Leuprolide Mesylate, and what is its action type?\\n\\n### Response:\\nLeuprolide Mesylate is a protein drug that acts as an agonist.<|endoftext|>'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_train_dataset = train_dataset.map(\n",
    "    generate_instruction_format,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    batched=False,\n",
    ")\n",
    "mapped_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.529600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.557700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.958000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.783500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=1.3351387580235798, metrics={'train_runtime': 66.4482, 'train_samples_per_second': 45.148, 'train_steps_per_second': 1.445, 'total_flos': 148906714632192.0, 'train_loss': 1.3351387580235798})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"cuda \", use_cuda)\n",
    "\n",
    "model.train()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,\n",
    "    logging_steps=20,\n",
    "    save_total_limit=2,\n",
    "    use_cpu=not(use_cuda)\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=mapped_train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check some outputs of our fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "question:  How many mechanisms of action does the drug Bazedoxifene have? \n",
      "\n",
      "model output:  The drug Bazenoproxine has 1 mechanism of action, Bredonomycin. \n",
      "\n",
      "expected output:  The drug Bazedoxifene has 1 mechanism of action, Estrogen receptor modulator.\n"
     ]
    }
   ],
   "source": [
    "index = 2\n",
    "\n",
    "question, response, expected_response = test_model(index, test_dataset, model, tokenizer)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"question: \", question, \"\\n\")\n",
    "print(\"model output: \", response, \"\\n\")\n",
    "print(\"expected output: \", expected_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we cannot check every output individually. \n",
    "\n",
    "\n",
    "Instead, we use **ROUGE-L** as a metric to evaluate the fine-tuned model on the test dataset.\n",
    "\n",
    "The **ROUGE-L** score is based on the longest common subsequence (LCS) between the generated and the reference text. \n",
    "The LCS is the longest sequence of words that appear in order in both generated and reference text. \n",
    "The words do **NOT** need to be contiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "- *Reference text:* \"The **kid** is **playing** with **the cat**\"\n",
    "\n",
    "- *Generated text:* \"kid playing the cat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that case, the *LCS* is \"kid playing the cat\". \n",
    "\n",
    "To calculate the ROUGE-L score, we need the following information:\n",
    "\n",
    "- length(LCS) = 4\n",
    "- length(reference text) = 7\n",
    "- length(generated text) = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one can calculate Recall, Precision, and F1 Score\n",
    "\n",
    "- *ROUGE-L Recall* = $\\frac{\\text{length(LCS)}}{\\text{length(reference text)}} = \\frac{4}{7} \\approx 0.57$\n",
    "\n",
    "- *ROUGE-L Precision* = $\\frac{\\text{length(LCS)}}{\\text{length(generated text)}} = \\frac{4}{4} = 1.0 $\n",
    "\n",
    "- *ROUGE-L F1 Score* = $\\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\approx 0.73 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_l score:  0.73\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "reference_text = [\"The kid is playing with the cat\"]\n",
    "generated_text = [\"kid playing the cat\"]\n",
    "\n",
    "results = rouge.compute(predictions=generated_text, references=reference_text)\n",
    "\n",
    "rouge_l = float(round(results[\"rougeL\"], 2))\n",
    "print(\"rouge_l score: \", rouge_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that generates model predictions on the test dataset and returns them together with the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(example):\n",
    "    instruction = \"You are a knowledgeable assistant. Answer this question truthfully!\"\n",
    "\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    # format the input the same way as during training\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        f\"{instruction}\\n\\n\"\n",
    "        \"### Input:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    # tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2,\n",
    "        )\n",
    "\n",
    "    # decode and clean up the response\n",
    "    predicted_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # remove the prompt from the response\n",
    "    predicted_output = predicted_output[len(prompt):]\n",
    "\n",
    "    return {\n",
    "        \"question\": example[\"question\"],\n",
    "        \"predicted_output\": predicted_output,\n",
    "        \"correct_output\": example[\"answer\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computational reasons, we only evaluate the model on a small subset of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_dataset.select(range(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:12<00:00,  2.45 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'How many mechanisms of action does the drug Bazedoxifene have?',\n",
       " 'predicted_output': 'The drug Bazenoproxine has 1 mechanism of action, Bredonomycin',\n",
       " 'correct_output': 'The drug Bazedoxifene has 1 mechanism of action, Estrogen receptor modulator.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_dataset = test_ds.map(get_predictions, batched=False, remove_columns=test_ds.column_names)\n",
    "predictions_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L (F1): 67.14%\n"
     ]
    }
   ],
   "source": [
    "predictions = predictions_dataset[\"predicted_output\"]\n",
    "references = predictions_dataset[\"correct_output\"]\n",
    "\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(f\"ROUGE-L (F1): {results['rougeL']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to SmolLM2 base model (not fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload SmolLM2 base model\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "untrained_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_untrained_predictions(example):\n",
    "    instruction = \"You are a knowledgeable assistant. Answer this question truthfully!\"\n",
    "\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    # Format the input the same way as during training\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        f\"{instruction}\\n\\n\"\n",
    "        \"### Input:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = untrained_model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.2) \n",
    "    \n",
    "    predicted_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predicted_output = predicted_output[len(prompt):]\n",
    "\n",
    "    return {\n",
    "        \"question\": example[\"question\"],\n",
    "        \"predicted_output\": predicted_output,\n",
    "        \"correct_output\": example[\"answer\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:12<00:00,  2.50 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'How many mechanisms of action does the drug Bazedoxifene have?',\n",
       " 'predicted_output': 'The answer is 10, which means that there will be one mechanism for each molecule in your',\n",
       " 'correct_output': 'The drug Bazedoxifene has 1 mechanism of action, Estrogen receptor modulator.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untrained_predictions_dataset = test_ds.map(get_untrained_predictions, batched=False, remove_columns=test_ds.column_names)\n",
    "untrained_predictions_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L (F1): 13.97%\n"
     ]
    }
   ],
   "source": [
    "predictions = untrained_predictions_dataset[\"predicted_output\"]\n",
    "references = untrained_predictions_dataset[\"correct_output\"]\n",
    "\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(f\"ROUGE-L (F1): {results['rougeL']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROUGE-L (F1) of fine-tuned model: ~ 70% :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
