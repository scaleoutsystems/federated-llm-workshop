{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install trl evaluate rouge_score transformers peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edgelab/llm-workshop/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from trl import SFTTrainer\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the *CARDBiomedBench* dataset from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more information about the dataset at **[Hugging Face](https://huggingface.co/datasets/NIH-CARD/CARDBiomedBench)**, or by reading the [paper](https://www.biorxiv.org/content/10.1101/2025.01.15.633272v2.full.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['uuid', 'template_uuid', 'question', 'answer', 'bio_category', 'reasoning_category'],\n",
       "        num_rows: 58079\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['uuid', 'template_uuid', 'question', 'answer', 'bio_category', 'reasoning_category'],\n",
       "        num_rows: 10148\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"NIH-CARD/CARDBiomedBench\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains question-answer pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What genes does Olaparib target when it's used to treat cancer?\n",
      "Answer  :  When used to treat cancer, the drug Olaparib targets the genes PARP2, PARP1, and PARP3.\n"
     ]
    }
   ],
   "source": [
    "question = dataset[\"train\"][0][\"question\"]\n",
    "answer = dataset[\"train\"][0][\"answer\"]\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer  : \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's only use data samples related to category \"Pharmacology\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.filter(lambda x: x[\"bio_category\"] == \"Pharmacology\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computational reasons, we select a smaller subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "# select a smaller subset\n",
    "train_dataset = train_dataset.shuffle(seed=42).select(range(1000))\n",
    "test_dataset = test_dataset.shuffle(seed=42).select(range(200))\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What type of molecule is Leuprolide Mesylate, and what is its action type?\n",
      "Answer  :  Leuprolide Mesylate is a protein drug that acts as an agonist.\n"
     ]
    }
   ],
   "source": [
    "question = train_dataset[0][\"question\"]\n",
    "answer = train_dataset[0][\"answer\"]\n",
    "print(\"Question: \", question)\n",
    "print(\"Answer  : \", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the [SmolLM2](https://huggingface.co./HuggingFaceTB/SmolLM2-135M) decoder model, developed by Hugging Face. The SmolLM2 models come in three sizes (135M, 360M, and 1.7B parameters) and are developed to solve a wide range of tasks while being lightweight enough to run on-device.\n",
    "Here, we choose the 135M parameter model for computational reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the model and tokenizer through Hugging Face:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(model).to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # set end-of-sequence token as padding token\n",
    "model.config.pad_token_id = model.config.eos_token_id  # tell model which token to use for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134515008"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters() # 134,515,008 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate an example output of our base LLM.\n",
    "\n",
    "Note that the model is not instruction-tuned (unlike ChatGPT). It is only trained to predict the next token in a sequence and is less useful for interactive tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Sweden is  the city of Stockholm. The city is located in the middle of the country, and is the\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"The capital of Sweden is \", return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], \n",
    "                         attention_mask=inputs[\"attention_mask\"],\n",
    "                         pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though 135M parameters is relatively \"small\" for an LLM, we want to further reduce the number of trainable parameters through LoRA. \n",
    "\n",
    "This becomes even more necessary when we choose to train larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 460,800 || all params: 134,975,808 || trainable%: 0.3414\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the SmolLM2 base model on some questions from our dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(index, dataset, model, tokenizer):\n",
    "    data = dataset[index]\n",
    "    question = data[\"question\"]\n",
    "\n",
    "    instruction = \"You are a knowledgeable assistant. Answer this question truthfully!\"\n",
    "\n",
    "    # Format the input into instruction format\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        f\"{instruction}\\n\\n\"\n",
    "        \"### Input:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate response\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=100,    \n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "\n",
    "    # decode the response & remove special tokens\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # remove prompt from response\n",
    "    response = response[len(prompt):]\n",
    "\n",
    "    expected_response = data[\"answer\"]\n",
    "\n",
    "    return question, response, expected_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: \n",
      " What type of molecule is Leuprolide Mesylate, and what is its action type? \n",
      "\n",
      "model output: \n",
      " Leptolipin (LPL) has an active site that binds to the hydrophobic side chains in phospholipids such as cholesterol or triglycerides. This binding allows for lipophilic molecules like leprous bacteria to be transported into cells where they can multiply rapidly without being destroyed by phagocytes. The LPO-binding sites on these bacterial membranes also allow them to bind with other proteins which then act upon their own membrane lipids causing cell death when released from host tissues through endocytosis processes.\n",
      "\n",
      "expected output: \n",
      " Leuprolide Mesylate is a protein drug that acts as an agonist.\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "\n",
    "question, response, expected_response = test_model(index, train_dataset, model, tokenizer)\n",
    "\n",
    "print(\"question: \\n\", question, \"\\n\")\n",
    "print(\"model output: \\n\", response)\n",
    "print(\"expected output: \\n\", expected_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralized Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to fine-tune the model on the train dataset. For this, we convert the training data to instruction format. This is the correct format for generative question-answering tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instruction_format(example):\n",
    "    question = example[\"question\"]\n",
    "    answer = example[\"answer\"]\n",
    "\n",
    "    instruction = \"You are a knowledgeable assistant. Answer this question truthfully!\"\n",
    "\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        f\"{instruction.strip()}\\n\\n\"\n",
    "        \"### Input:\\n\"\n",
    "        f\"{question.strip()}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "        f\"{answer.strip()}\" + tokenizer.eos_token\n",
    "    )\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 11386.74 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': '### Instruction:\\nYou are a knowledgeable assistant. Answer this question truthfully!\\n\\n### Input:\\nWhat type of molecule is Leuprolide Mesylate, and what is its action type?\\n\\n### Response:\\nLeuprolide Mesylate is a protein drug that acts as an agonist.<|endoftext|>'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_train_dataset = train_dataset.map(\n",
    "    generate_instruction_format,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    batched=False,\n",
    ")\n",
    "mapped_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train dataset to ChatML: 100%|██████████| 1000/1000 [00:00<00:00, 56834.16 examples/s]\n",
      "Adding EOS to train dataset: 100%|██████████| 1000/1000 [00:00<00:00, 40950.80 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 1000/1000 [00:00<00:00, 5216.56 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 1000/1000 [00:00<00:00, 304840.76 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.549100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.942700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.779400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=1.3288022379080455, metrics={'train_runtime': 65.8502, 'train_samples_per_second': 45.558, 'train_steps_per_second': 1.458, 'total_flos': 148906714632192.0, 'train_loss': 1.3288022379080455})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"cuda \", use_cuda)\n",
    "\n",
    "model.train()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # output_dir=\"qa-finetuned\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,\n",
    "    logging_steps=20,\n",
    "    save_total_limit=2,\n",
    "    use_cpu=not(use_cuda)\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=mapped_train_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check some outputs of our fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "question:  What mechanism type does the drug Avutometinib use? \n",
      "\n",
      "model output:  The drug Avutmetinib is an inhibitor. \n",
      "\n",
      "expected output:  The drug Avutometinib is an inhibitor.\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "\n",
    "question, response, expected_response = test_model(index, train_dataset, model, tokenizer)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"question: \", question, \"\\n\")\n",
    "print(\"model output: \", response, \"\\n\")\n",
    "print(\"expected output: \", expected_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we cannot check every output individually. \n",
    "\n",
    "\n",
    "Instead, we use **ROUGE-L** as a metric to evaluate the fine-tuned model on the test dataset.\n",
    "\n",
    "The **ROUGE-L** score is based on the longest common subsequence (LCS) between the generated and the reference text. \n",
    "The LCS is the longest sequence of words that appear in order in both generated and reference text. \n",
    "The words do **NOT** need to be contiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "\n",
    "- *Reference text:* \"The **kid** is **playing** with **the cat**\"\n",
    "\n",
    "- *Generated text:* \"kid playing the cat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In that case, the *LCS* is \"kid playing the cat\". \n",
    "\n",
    "To calculate the ROUGE-L score, we need the following information:\n",
    "\n",
    "- length(LCS) = 4\n",
    "- length(reference text) = 7\n",
    "- length(generated text) = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, one can calculate Recall, Precision, and F1 Score\n",
    "\n",
    "- *ROUGE-L Recall* = $\\frac{\\text{length(LCS)}}{\\text{length(reference text)}} = \\frac{4}{7} \\approx 0.57$\n",
    "\n",
    "- *ROUGE-L Precision* = $\\frac{\\text{length(LCS)}}{\\text{length(generated text)}} = \\frac{4}{4} = 1.0 $\n",
    "\n",
    "- *ROUGE-L F1 Score* = $\\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} = \\approx 0.727 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7272727272727273)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "reference_text = [\"The kid is playing with the cat\"]\n",
    "generated_text = [\"kid playing the cat\"]\n",
    "\n",
    "results = rouge.compute(predictions=generated_text, references=reference_text)\n",
    "results[\"rougeL\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that transforms the model predictions and the expected output into the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(example):\n",
    "    instruction = \"You are a knowledgeable assistant. Answer this question truthfully!\"\n",
    "\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    # Format the input the same way as during training\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        f\"{instruction}\\n\\n\"\n",
    "        \"### Input:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            # max_new_tokens=100,\n",
    "            # temperature=0.1,\n",
    "            # top_p=0.9,\n",
    "            # do_sample=True,\n",
    "            # pad_token_id=tokenizer.eos_token_id,\n",
    "            # repetition_penalty=1.2,\n",
    "        )\n",
    "\n",
    "    # Decode and clean up the response\n",
    "    predicted_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the prompt from the response\n",
    "    predicted_output = predicted_output[len(prompt):]\n",
    "\n",
    "    return {\n",
    "        \"question\": example[\"question\"],\n",
    "        \"predicted_output\": predicted_output,\n",
    "        \"correct_output\": example[\"answer\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computational reasons, we only evaluate the model on a small subset of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = test_dataset.select(range(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/30 [00:00<?, ? examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:   3%|▎         | 1/30 [00:00<00:14,  1.95 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:   7%|▋         | 2/30 [00:00<00:10,  2.67 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  10%|█         | 3/30 [00:01<00:11,  2.31 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  13%|█▎        | 4/30 [00:01<00:10,  2.44 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  17%|█▋        | 5/30 [00:02<00:11,  2.26 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  20%|██        | 6/30 [00:02<00:09,  2.59 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  23%|██▎       | 7/30 [00:02<00:09,  2.46 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  27%|██▋       | 8/30 [00:03<00:08,  2.62 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  30%|███       | 9/30 [00:03<00:07,  2.87 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  33%|███▎      | 10/30 [00:03<00:07,  2.74 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  37%|███▋      | 11/30 [00:04<00:06,  2.89 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  40%|████      | 12/30 [00:04<00:06,  2.59 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  43%|████▎     | 13/30 [00:05<00:06,  2.61 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  47%|████▋     | 14/30 [00:05<00:06,  2.62 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  50%|█████     | 15/30 [00:05<00:05,  2.63 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  53%|█████▎    | 16/30 [00:06<00:05,  2.69 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  57%|█████▋    | 17/30 [00:06<00:05,  2.48 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  60%|██████    | 18/30 [00:07<00:04,  2.44 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  63%|██████▎   | 19/30 [00:07<00:04,  2.41 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  67%|██████▋   | 20/30 [00:07<00:03,  2.63 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  70%|███████   | 21/30 [00:08<00:03,  2.69 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  73%|███████▎  | 22/30 [00:08<00:03,  2.49 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  77%|███████▋  | 23/30 [00:09<00:02,  2.48 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  80%|████████  | 24/30 [00:09<00:02,  2.63 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  83%|████████▎ | 25/30 [00:09<00:01,  2.69 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  87%|████████▋ | 26/30 [00:10<00:01,  2.58 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  90%|█████████ | 27/30 [00:10<00:01,  2.38 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  93%|█████████▎| 28/30 [00:11<00:00,  2.25 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map:  97%|█████████▋| 29/30 [00:11<00:00,  2.32 examples/s]Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Map: 100%|██████████| 30/30 [00:11<00:00,  2.54 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'How many mechanisms of action does the drug Bazedoxifene have?',\n",
       " 'predicted_output': 'The drug Bazedoxifene has 1 mechanism of action, Bredin-type inhibitor',\n",
       " 'correct_output': 'The drug Bazedoxifene has 1 mechanism of action, Estrogen receptor modulator.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_dataset = test_ds.map(get_predictions, batched=False, remove_columns=test_ds.column_names)\n",
    "predictions_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L (F1): 75.46%\n"
     ]
    }
   ],
   "source": [
    "predictions = predictions_dataset[\"predicted_output\"]\n",
    "references = predictions_dataset[\"correct_output\"]\n",
    "\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(f\"ROUGE-L (F1): {results['rougeL']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to SmolLM2 base model (not fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload SmolLM2 base model\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "untrained_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_untrained_predictions(example):\n",
    "    instruction = \"You are a knowledgeable assistant. Answer this question truthfully!\"\n",
    "\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    # Format the input the same way as during training\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        f\"{instruction}\\n\\n\"\n",
    "        \"### Input:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = untrained_model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    repetition_penalty=1.2) \n",
    "    \n",
    "    predicted_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    predicted_output = predicted_output[len(prompt):]\n",
    "\n",
    "    return {\n",
    "        \"question\": example[\"question\"],\n",
    "        \"predicted_output\": predicted_output,\n",
    "        \"correct_output\": example[\"answer\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 30/30 [00:52<00:00,  1.74s/ examples]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'How many mechanisms of action does the drug Bazedoxifene have?',\n",
       " 'predicted_output': 'The answer is 10, because there were two actions in the experiment and one was not observed (the other being an unknown). The number that you get from your calculator should be equal to or greater than 256.\\n\\n47893',\n",
       " 'correct_output': 'The drug Bazedoxifene has 1 mechanism of action, Estrogen receptor modulator.'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untrained_predictions_dataset = test_ds.map(get_untrained_predictions, batched=False, remove_columns=test_ds.column_names)\n",
    "untrained_predictions_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L (F1): 6.94%\n"
     ]
    }
   ],
   "source": [
    "predictions = untrained_predictions_dataset[\"predicted_output\"]\n",
    "references = untrained_predictions_dataset[\"correct_output\"]\n",
    "\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(f\"ROUGE-L (F1): {results['rougeL']:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
