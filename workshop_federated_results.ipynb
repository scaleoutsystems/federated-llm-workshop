{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from peft import get_peft_model, get_peft_model_state_dict, set_peft_model_state_dict, LoraConfig, TaskType\n",
    "from trl import SFTTrainer\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by splitting up the dataset into 5 different partitions. Each hospital has access to only one biomedical category of the CARDBiomedBench dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"NIH-CARD/CARDBiomedBench\")\n",
    "\n",
    "biomed_categories = [\n",
    "    \"Drug Gene Relations\",\n",
    "    \"Pharmacology\",\n",
    "    \"Drug Meta\",\n",
    "    \"SNP Disease Relations\",\n",
    "    \"SMR Gene Disease Relations\"\n",
    "]\n",
    "\n",
    "train_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "for category in biomed_categories:\n",
    "    filtered_train_ds = dataset[\"train\"].filter(lambda x: x[\"bio_category\"] == category)\n",
    "    filtered_test_ds = dataset[\"test\"].filter(lambda x: x[\"bio_category\"] == category)\n",
    "    filtered_train_ds = filtered_train_ds.shuffle(seed=42).select(range(1_000))\n",
    "    filtered_test_ds = filtered_test_ds.shuffle(seed=42).select(range(100))\n",
    "    train_datasets.append(filtered_train_ds)\n",
    "    test_datasets.append(filtered_test_ds)\n",
    "    \n",
    "test_dataset = concatenate_datasets(test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the case where hospitals are not collaborating. Each hospital fine-tunes their own LLM in a standard, centralized way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hospital 1: Centralized model (20% train data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Centralized Case](figures/centralized_case.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralized_dataset = train_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "model = AutoModelForCausalLM.from_pretrained(model).to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "model.config.pad_token_id = model.config.eos_token_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centralized Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_train_dataset = centralized_dataset.map(\n",
    "    generate_instruction_format,\n",
    "    remove_columns=centralized_dataset.column_names,\n",
    "    batched=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-4,\n",
    "    logging_steps=20,\n",
    "    save_total_limit=2,\n",
    "    use_cpu=not(use_cuda)\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=mapped_train_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(example):\n",
    "    instruction = \"You are a knowledgeable assistant. Answer this question truthfully!\"\n",
    "\n",
    "    question = example[\"question\"]\n",
    "\n",
    "    # format the input the same way as during training\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        f\"{instruction}\\n\\n\"\n",
    "        \"### Input:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # generate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "        )\n",
    "\n",
    "    predicted_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # removes the prompt from the response\n",
    "    predicted_output = predicted_output[len(prompt):]\n",
    "\n",
    "    return {\n",
    "        \"question\": example[\"question\"],\n",
    "        \"predicted_output\": predicted_output,\n",
    "        \"correct_output\": example[\"answer\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cached_predictions(dataset, filename_predictions, filename_references):\n",
    "    if os.path.exists(filename_predictions) and os.path.exists(filename_references):\n",
    "        print(\"Loading cached predictions\")\n",
    "        predictions = load_list(filename_predictions)\n",
    "        references = load_list(filename_references)\n",
    "    else:\n",
    "        print(\"Generating new predictions\")\n",
    "        predictions_dataset = dataset.map(\n",
    "            get_predictions,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        \n",
    "        predictions = predictions_dataset[\"predicted_output\"]\n",
    "        references = predictions_dataset[\"correct_output\"]\n",
    "        \n",
    "        save_list(predictions, filename_predictions)\n",
    "        save_list(references, filename_references)\n",
    "    \n",
    "    return predictions, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, references = get_cached_predictions(test_dataset, \"model_predictions/predictions_centralized.json\", \"model_predictions/references_centralized.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralized_results = compute_rouge(predictions, references)\n",
    "\n",
    "rouge_l = round(centralized_results[\"rougeL\"], 2)\n",
    "rouge_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the 5 hospitals decide to collaboratively fine-tune an LLM through federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Fine-tuning (5 hospitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Centralized Case](figures/federated_case.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "path = \"models/federated_finetuned_model\"\n",
    "model = load_lora_parameters(path).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, references = get_cached_predictions(test_dataset, \"model_predictions/predictions_federated.json\", \"model_predictions/references_federated.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_results = compute_rouge(predictions, references)\n",
    "\n",
    "rouge_l = round(federated_results[\"rougeL\"], 2)\n",
    "rouge_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base model (not fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  \n",
    "model.config.pad_token_id = model.config.eos_token_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, references = get_cached_predictions(test_dataset, \"model_predictions/predictions_base_model.json\", \"model_predictions/references_base_model.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untrained_results = compute_rouge(predictions, references)\n",
    "\n",
    "rouge_l = round(untrained_results[\"rougeL\"], 2)\n",
    "rouge_l\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROUGE comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [untrained_results,\n",
    "           centralized_results, \n",
    "           federated_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_l_scores = [data['rougeL'] for data in results]\n",
    "\n",
    "labels = [\"Base model\", \"Centralized (1 Hospital)\", \"Federated (5 Hospitals)\"]\n",
    "\n",
    "colors = [\"tab:blue\", \"tab:orange\", \"tab:green\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE L Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "bars = plt.bar(labels, rouge_l_scores, color=colors)\n",
    "\n",
    "plt.title('Rouge-L Score Comparison')\n",
    "plt.ylabel('Rouge-L Score')\n",
    "plt.ylim(0, 0.6)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "Federated fine-tuning of LLMs unlocks (private) data, that otherwise would not be usable. The outcome is a much more capable model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
